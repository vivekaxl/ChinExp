{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "import scipy.stats as ss\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy import median\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/chsu6/scout/Data\"\n",
    "normalized_data_dir = \"/home/chsu6/scout/NormalizedData\"\n",
    "solution_dir = \"/home/chsu6/scout/solution\"\n",
    "\n",
    "\n",
    "file_list = os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_mape(y, y_predicted):\n",
    "    return np.mean(abs(y-y_predicted)/(y+0.000001))\n",
    "\n",
    "def get_score_function(name):\n",
    "    if name == \"mse\":\n",
    "        return mean_squared_error\n",
    "    elif name == \"mape\":\n",
    "        return score_mape\n",
    "\n",
    "def random_strategy(select_percentage, df_candidate, df_test, score_func=get_score_function(\"mse\")):\n",
    "    select_index = random.sample(list(df_candidate.index), max(5, int(select_percentage*len(df_candidate.index))))    \n",
    "    df_training = df_candidate.ix[select_index, :]\n",
    "    clf = tree.DecisionTreeRegressor()\n",
    "    clf.fit(df_training.ix[:, :-1], df_training.ix[:, -1])\n",
    "    test_predicted = clf.predict(df_test.ix[:, :-1])\n",
    "    return (score_func(df_test.ix[:, -1], test_predicted), select_index)\n",
    "\n",
    "def brute_force_random_strategy(select_percentage, df_candidate, df_test, num_iter=1000, score_name=\"mse\"):\n",
    "    score_func = get_score_function(score_name)\n",
    "    score_records = []\n",
    "    index_records = {}\n",
    "    for i in range(num_iter):\n",
    "        (score, select_index) = random_strategy(select_percentage, df_candidate, df_test, score_func=score_func)\n",
    "        score_records.append(score)\n",
    "        index_records[i] = select_index\n",
    "    \n",
    "    return (score_records, index_records)\n",
    "    \n",
    "def create_filtered_df(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df[df.ix[:, -1] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_name = \"mape\"\n",
    "output_dir = os.path.join(solution_dir, \"test1\")\n",
    "for f in sorted(file_list):\n",
    "    \n",
    "    print(\"[{}]\".format(f))\n",
    "    \n",
    "    # initialize setting\n",
    "    run_output_dir = os.path.join(output_dir, f.split(\".\")[0])\n",
    "    original_file = os.path.join(run_output_dir, \"full.csv\")\n",
    "    candidate_file = os.path.join(run_output_dir, \"candidate.csv\")\n",
    "    testing_file = os.path.join(run_output_dir, \"testing.csv\")\n",
    "    statistics_file = os.path.join(run_output_dir, \"statistics.txt\")\n",
    "    os.makedirs(run_output_dir, exist_ok=True)\n",
    "    candidate_percentage = 0.6\n",
    "    select_percentage = 0.1\n",
    "    num_iter = 10000\n",
    "    \n",
    "    # initialize data\n",
    "    df = create_filtered_df(os.path.join(data_dir, f))\n",
    "    index_candidate = random.sample(list(df.index), int(candidate_percentage*len(df.index)))\n",
    "    index_test = [row for row in df.index if row not in index_candidate]\n",
    "    assert len(index_candidate) + len(index_test) == len(df)\n",
    "    df.ix[index_candidate, :].to_csv(candidate_file, index=False)\n",
    "    df.ix[index_test, :].to_csv(testing_file, index=False)\n",
    "    \n",
    "    # random sampling\n",
    "    num_top_score = 10\n",
    "    (score_records, index_records) = brute_force_random_strategy(select_percentage, df.ix[index_candidate, :], df.ix[index_test, :], num_iter=num_iter, score_name=score_name)\n",
    "    score_ranks = ss.rankdata(score_records)\n",
    "    best_n_index = np.argpartition(score_records, num_top_score)[:num_top_score] \n",
    "    worst_n_index = np.argpartition(score_records, -num_top_score)[-num_top_score:] \n",
    "    \n",
    "    # best cases\n",
    "    for _ in best_n_index:\n",
    "        best_index = score_ranks[_]\n",
    "        output = os.path.join(run_output_dir, \"best_{}_{:.4f}.csv\".format(int(best_index), score_records[_]))\n",
    "        df.ix[index_records[_], :].to_csv(output, index=False)\n",
    "    \n",
    "    # worst vases\n",
    "    for _ in worst_n_index:\n",
    "        worst_index = score_ranks[_]\n",
    "        output = os.path.join(run_output_dir, \"worst_{}_{:.4f}.csv\".format(len(score_ranks)-int(worst_index)+1, score_records[_]))\n",
    "        df.ix[index_records[_], :].to_csv(output, index=False)\n",
    "    \n",
    "    # write files\n",
    "    df.to_csv(original_file, index=False)\n",
    "    \n",
    "    # statistics\n",
    "    with open(statistics_file, \"w\") as output_file:\n",
    "        output_file.write(\"total_points={}\\n\".format(len(df)))\n",
    "        output_file.write(\"candidate_points={}\\n\".format(len(index_candidate)))\n",
    "        output_file.write(\"testing_points={}\\n\".format(len(index_test)))\n",
    "        output_file.write(\"candidate_percentage={}\\n\".format(candidate_percentage))\n",
    "        output_file.write(\"select_percentage={}\\n\".format(select_percentage))\n",
    "        output_file.write(\"num_iter={}\\n\".format(num_iter))\n",
    "        output_file.write(\"\\n\")\n",
    "        \n",
    "        output_file.write(\"MIN={:.4f}\\n\".format(min(score_records)))\n",
    "        output_file.write(\"MAX={:.4f}\\n\".format(max(score_records)))\n",
    "        output_file.write(\"MEAN={:.4f}\\n\".format(np.mean(score_records)))\n",
    "        output_file.write(\"MEDIAN={:.4f}\\n\".format(np.median(score_records)))\n",
    "        output_file.write(\"STD={:.4f}\\n\".format(np.std(score_records)))\n",
    "        output_file.write(\"\\n\")\n",
    "        for score in sorted(score_records):\n",
    "            output_file.write(\"{:.4f}\".format(score))\n",
    "            output_file.write(\"\\n\")\n",
    "    \n",
    "    print(\"\\t* The min {}={:.4f}\".format(score_name.upper(), min(score_records)))\n",
    "    print(\"\\t* The max {}={:.4f}\".format(score_name.upper(), max(score_records)))\n",
    "    print(\"\\t* The mean {}={:.4f}\".format(score_name.upper(), np.mean(score_records)))\n",
    "    print(\"\\t* The median {}={:.4f}\".format(score_name.upper(), np.median(score_records)))\n",
    "    print(\"\\t* The std {}={:.4f}\".format(score_name.upper(), np.std(score_records)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20160324'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.now().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}